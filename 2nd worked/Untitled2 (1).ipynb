{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "YoHunp7csTbn",
        "outputId": "18060ffb-4c33-4fae-e8d7-e5d3069b3799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "no such file: 'Resume.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0dc0ffcd2214>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Example Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Resume.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-0dc0ffcd2214>\u001b[0m in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Open the PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2916\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: 'Resume.pdf'"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)  # Open the PDF\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"  # Extract text from each page\n",
        "    return text\n",
        "\n",
        "# Example Usage\n",
        "pdf_file = \"Resume.pdf\"\n",
        "text = extract_text_from_pdf(pdf_file)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained model and vectorizer\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "\n",
        "# Preprocess text (cleaning function)\n",
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words(\"english\"))  # Load stopwords\n",
        "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Function to split large text into smaller chunks\n",
        "def split_text(text, chunk_size=50):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Function to detect plagiarism for long texts\n",
        "def detect_large_text(input_text, similarity_threshold=0.7):\n",
        "    chunks = split_text(input_text)  # Split large text into smaller chunks\n",
        "    processed_chunks = [preprocess_text(chunk) for chunk in chunks]  # Preprocess chunks\n",
        "    vectorized_chunks = tfidf_vectorizer.transform(processed_chunks)  # Convert to TF-IDF\n",
        "\n",
        "    predictions = model.predict(vectorized_chunks)  # Get model predictions\n",
        "\n",
        "    # Cosine similarity check\n",
        "    similarity_scores = cosine_similarity(vectorized_chunks, vectorized_chunks)\n",
        "    max_similarity = np.max(similarity_scores)\n",
        "\n",
        "    # If any chunk is plagiarized OR similarity is high, mark as plagiarism\n",
        "    if 1 in predictions or max_similarity > similarity_threshold:\n",
        "        return \"Plagiarism Detected\"\n",
        "    return \"No Plagiarism\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N2NrX5gsWxn",
        "outputId": "5b7f59e7-22e9-4a66-fe21-32c335f66a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(detect_large_text(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ckAWH2tVtv",
        "outputId": "6256a001-1af9-4a97-b87f-da0e9957fdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "\n",
        "# Preprocess text (cleaning function)\n",
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words(\"english\"))  # Load stopwords\n",
        "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Function to detect plagiarism for large texts\n",
        "def detect_large_text(input_text, similarity_threshold=0.7):\n",
        "    processed_text = preprocess_text(input_text)  # Preprocess entire text\n",
        "    vectorized_text = tfidf_vectorizer.transform([processed_text])  # Convert to TF-IDF\n",
        "\n",
        "    # Get model predictions\n",
        "    prediction = model.predict(vectorized_text)\n",
        "\n",
        "    # Cosine similarity check (assuming you compare with a database)\n",
        "    similarity_scores = cosine_similarity(vectorized_text, vectorized_text)\n",
        "    max_similarity = np.max(similarity_scores)\n",
        "\n",
        "    # If model detects plagiarism OR similarity is high, mark as plagiarism\n",
        "    if prediction[0] == 1 or max_similarity > similarity_threshold:\n",
        "        return \"Plagiarism Detected\"\n",
        "    return \"No Plagiarism\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ssbqFVs4tiCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34974dc-aecf-4dfd-a6be-c5c85a62d0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_text = \"\"\"Machine learning teaches computers to recognize patterns and make decisions automatically using data and algorithms. It can be broadly categorized into three types:\n",
        "Supervised Learning: Trains models on labeled data to predict or classify new, unseen data.\n",
        "Unsupervised Learning: Finds patterns or groups in unlabeled data, like clustering or dimensionality reduction.\n",
        "Reinforcement Learning: Learns through trial and error to maximize rewards, ideal for decision-making tasks.\"\"\"\n",
        "result = detect_large_text(input_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ9PUz-y-zyW",
        "outputId": "70dae9fc-0c1d-4af2-c262-b9858093f8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"I am an engineering student love to code.\"\"\"\n",
        "result = detect_large_text(input_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj4yN7DQ-2Zr",
        "outputId": "1a61f957-9cee-4544-a33a-6f2e30707d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POZpYh_H_QwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}