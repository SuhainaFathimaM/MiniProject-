{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoHunp7csTbn",
        "outputId": "f40032a9-300a-4fc2-a4e5-55ada36987e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "SUBASHINI M \n",
            "Pursuing a BE in Computer Science Engineering with a Passion for Innovation and \n",
            "Problem Solving \n",
            " \n",
            "692/01 V N R nagar \n",
            "Virudhachalam \n",
            "kalasaraswathi83@gmail.com \n",
            "Subashini M | Linkedin  \n",
            "9566161299 \n",
            " \n",
            "EDUCATION \n",
            "Higher Secondary (12th Grade) \n",
            "Fatima Matriculation Higher Secondary School \n",
            "Aggregate: 95.67% \n",
            "Bachelor of Engineering (BE) in Computer Science and \n",
            "Engineering \n",
            "Government College of Engineering, Salem \n",
            "Expected Graduation: 2026 \n",
            "PROJECTS \n",
            "Survey Form - Built Using HTML,CSS \n",
            "Color Flipper- Built Using HTML,CSS,JavaScript \n",
            "Digital Clock-Built Using HTML,CSS,JavacScript \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "SKILLS \n",
            "  \n",
            "Java Programming    \n",
            "C++   Programming \n",
            "SQL Programming   \n",
            "Web Development \n",
            " \n",
            "ADDITIONAL \n",
            "Presentation on Artificial \n",
            "Intelligence-AVS Engineering \n",
            "College \n",
            " \n",
            "Data Structures Certification â€“ \n",
            "Coursera \n",
            "LANGUAGES \n",
            "English ,Tamil \n",
            "HOBBIES \n",
            "Traveling , Problem Solving \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)  # Open the PDF\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"  # Extract text from each page\n",
        "    return text\n",
        "\n",
        "# Example Usage\n",
        "pdf_file = \"Resume.pdf\"\n",
        "text = extract_text_from_pdf(pdf_file)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained model and vectorizer\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "\n",
        "# Preprocess text (cleaning function)\n",
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words(\"english\"))  # Load stopwords\n",
        "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Function to split large text into smaller chunks\n",
        "def split_text(text, chunk_size=50):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Function to detect plagiarism for long texts\n",
        "def detect_large_text(input_text, similarity_threshold=0.7):\n",
        "    chunks = split_text(input_text)  # Split large text into smaller chunks\n",
        "    processed_chunks = [preprocess_text(chunk) for chunk in chunks]  # Preprocess chunks\n",
        "    vectorized_chunks = tfidf_vectorizer.transform(processed_chunks)  # Convert to TF-IDF\n",
        "\n",
        "    predictions = model.predict(vectorized_chunks)  # Get model predictions\n",
        "\n",
        "    # Cosine similarity check\n",
        "    similarity_scores = cosine_similarity(vectorized_chunks, vectorized_chunks)\n",
        "    max_similarity = np.max(similarity_scores)\n",
        "\n",
        "    # If any chunk is plagiarized OR similarity is high, mark as plagiarism\n",
        "    if 1 in predictions or max_similarity > similarity_threshold:\n",
        "        return \"Plagiarism Detected\"\n",
        "    return \"No Plagiarism\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N2NrX5gsWxn",
        "outputId": "5b7f59e7-22e9-4a66-fe21-32c335f66a0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(detect_large_text(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ckAWH2tVtv",
        "outputId": "6256a001-1af9-4a97-b87f-da0e9957fdc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ssbqFVs4tiCh"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}